import requests
import json
import pandas as pd
import os
import time
from tqdm import tqdm

# =============================
# è¨­å®š
# =============================
PROJECT = "TCGA-BRCA"  # ä»»æ„ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
DATA_TYPE = "Gene Expression Quantification"
SAVE_DIR = "GDC_download"
os.makedirs(SAVE_DIR, exist_ok=True)

# =============================
# 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
# =============================
print(f"ğŸ” {PROJECT} ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")

url = "https://api.gdc.cancer.gov/files"
params = {
    "filters": json.dumps({
        "op": "and",
        "content": [
            {"op": "in", "content": {"field": "cases.project.project_id", "value": [PROJECT]}},
            {"op": "in", "content": {"field": "data_type", "value": [DATA_TYPE]}},
            {"op": "in", "content": {"field": "data_format", "value": ["TXT", "TSV", "CSV", "htseq.counts"]}}
        ]
    }),
    "fields": "file_id,file_name,cases.submitter_id,data_format,data_type,cases.samples.sample_type",
    "format": "JSON",
    "size": "1000"
}

response = requests.get(url, params=params, timeout=60)
response.raise_for_status()
files = response.json()["data"]["hits"]
print(f"âœ… {len(files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

# =============================
# 2. å®‰å…¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•°
# =============================
def safe_download(file_id, save_path, max_retries=5, chunk_size=8192):
    """
    ä¸­æ–­æ™‚ã«å†è©¦è¡Œã—ã€éƒ¨åˆ†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å†é–‹ã‚‚å¯èƒ½ãªå®‰å…¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼
    """
    dl_url = f"https://api.gdc.cancer.gov/data/{file_id}"
    retries = 0

    while retries < max_retries:
        try:
            headers = {}
            downloaded_bytes = os.path.getsize(save_path) if os.path.exists(save_path) else 0

            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèªç”¨ HEAD ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            head = requests.head(dl_url, timeout=30)
            total_size = int(head.headers.get("content-length", 0))

            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Œå…¨ã«ã‚ã‚‹ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            if downloaded_bytes >= total_size > 0:
                print(f"âœ… æ—¢ã«å®Œå…¨ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {save_path}")
                return True

            # å†é–‹ç”¨ãƒ˜ãƒƒãƒ€
            if downloaded_bytes > 0:
                headers["Range"] = f"bytes={downloaded_bytes}-"

            with requests.get(dl_url, stream=True, headers=headers, timeout=120) as r:
                if r.status_code == 416:
                    print(f"âš ï¸ {file_id}: ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã™ã§ã«å®Œå…¨ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ (416)ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    return True

                r.raise_for_status()
                total_to_download = int(r.headers.get("content-length", 0)) + downloaded_bytes

                mode = "ab" if downloaded_bytes > 0 else "wb"
                with open(save_path, mode) as f:
                    pbar = tqdm(
                        total=total_to_download,
                        initial=downloaded_bytes,
                        unit="B",
                        unit_scale=True,
                        desc=os.path.basename(save_path)
                    )

                    for chunk in r.iter_content(chunk_size=chunk_size):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
                    pbar.close()

            return True  # æ­£å¸¸çµ‚äº†

        except requests.exceptions.HTTPError as e:
            if e.response is not None and e.response.status_code == 416:
                print(f"âš ï¸ {file_id}: 416ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨ï¼‰ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return True
            retries += 1
            print(f"âš ï¸ HTTPã‚¨ãƒ©ãƒ¼ {e}. å†è©¦è¡Œ {retries}/{max_retries}")
            time.sleep(5 * retries)

        except (requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                requests.exceptions.ChunkedEncodingError) as e:
            retries += 1
            wait = 5 * retries
            print(f"âš ï¸ æ¥ç¶šä¸­æ–­ ({retries}/{max_retries}) - {e}. {wait}så¾Œã«å†è©¦è¡Œã€‚")
            time.sleep(wait)

    print(f"âŒ {file_id} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    return False



# =============================
# 3. ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =============================
records = []
for f in files:
    file_id = f["file_id"]
    file_name = f["file_name"]
    sample_id = f["cases"][0]["submitter_id"] if f["cases"] else "Unknown"
    local_path = os.path.join(SAVE_DIR, file_name)

    print(f"\nâ¬‡ï¸ {file_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    ok = safe_download(file_id, local_path)

    records.append({
        "file_id": file_id,
        "file_name": file_name,
        "sample_id": sample_id,
        "data_type": f.get("data_type"),
        "data_format": f.get("data_format"),
        "local_path": local_path,
        "status": "success" if ok else "failed"
    })

# =============================
# 4. çµæœã‚’DataFrameåŒ–ãƒ»ä¿å­˜
# =============================
df = pd.DataFrame(records)
meta_path = os.path.join(SAVE_DIR, f"{PROJECT}_metadata.csv")
df.to_csv(meta_path, index=False)
print(f"\nğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {meta_path}")

# å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
failed = df[df["status"] == "failed"]
if not failed.empty:
    print("\nâš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™:")
    print(failed[["file_id", "file_name"]])
